{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488466c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"skin_tone.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1z6Lq_jkMTalP4mHFs1Sq_vb2RadFSb8Q\n",
    "\"\"\"\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "# Upload archive.zip manually via Colab file browser first\n",
    "zip_path = \"archive.zip\"\n",
    "extract_path = \"/content/skintone_dataset\"\n",
    "\n",
    "with ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "# Check structure\n",
    "os.listdir(extract_path)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "train_path = '/content/skintone_dataset/train'\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * np.exp(-0.1)\n",
    "\n",
    "def build_skin_tone_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(224, 224, 3)),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(3, activation='softmax')  # 3 classes: Black, Brown, White\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_skin_tone_model()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    LearningRateScheduler(lr_scheduler),\n",
    "    EarlyStopping(patience=5, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "val_loss, val_acc = model.evaluate(val_generator, verbose=1)\n",
    "print(f\"Validation Accuracy: {val_acc:.2%}\")\n",
    "\n",
    "# Define the path inside your Google Drive\n",
    "save_path = \"vgg16_skin_tone.h5\"\n",
    "\n",
    "# Save the model\n",
    "model.save(save_path)\n",
    "\n",
    "print(f\"Model saved to: {save_path}\")\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ========== 1. Extract Dataset ==========\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "zip_path = \"archive.zip\"  # Upload via Colab sidebar\n",
    "extract_path = \"/content/skintone_dataset\"\n",
    "\n",
    "with ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(\"Extracted folders/files:\", os.listdir(extract_path))\n",
    "\n",
    "\n",
    "# ========== 2. Import Libraries ==========\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# ========== 3. Data Augmentation ==========\n",
    "train_path = os.path.join(extract_path, 'train')\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    brightness_range=[0.7, 1.4],\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "\n",
    "# ========== 4. Handle Class Imbalance ==========\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "# ========== 5. Load Pretrained VGG16 Model ==========\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "base_model.trainable = False  # Freeze pretrained layers\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(3, activation='softmax')(x)  # 3 skin tone classes\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# ========== 6. Callbacks ==========\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "# ========== 7. Training ==========\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=30,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# ========== 8. Evaluation ==========\n",
    "val_loss, val_acc = model.evaluate(val_generator, verbose=1)\n",
    "print(f\"âœ… Final Validation Accuracy: {val_acc:.2%}\")\n",
    "\n",
    "\n",
    "# ========== 9. Visualization ==========\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "zip_path = \"archive.zip\"  # replace with new uploaded zip filename\n",
    "extract_path = \"/content/skintone_dataset\"\n",
    "\n",
    "with ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(\"âœ… Extracted folders:\", os.listdir(extract_path))\n",
    "\n",
    "import os\n",
    "print(\"Subfolders in /train:\", os.listdir(os.path.join(extract_path, \"train\")))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from collections import Counter\n",
    "\n",
    "# ===========================================================\n",
    "# 3. Data Generators\n",
    "# ===========================================================\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    rotation_range=30,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    brightness_range=[0.7, 1.4],\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    f\"{extract_path}/train\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    f\"{extract_path}/train\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Class indices:\", train_gen.class_indices)\n",
    "print(\"ðŸ“Š Train distribution:\", dict(Counter(train_gen.classes)))\n",
    "print(\"ðŸ“Š Val distribution:\", dict(Counter(val_gen.classes)))\n",
    "\n",
    "# ===========================================================\n",
    "# 4. Compute Class Weights\n",
    "# ===========================================================\n",
    "class_weights = dict(enumerate(compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_gen.classes),\n",
    "    y=train_gen.classes\n",
    ")))\n",
    "\n",
    "# ===========================================================\n",
    "# 5. Build VGG16 Model\n",
    "# ===========================================================\n",
    "def build_vgg_model(input_shape=(224, 224, 3), num_classes=3):\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=input_shape))\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = Flatten()(base_model.output)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_vgg_model()\n",
    "model.summary()\n",
    "\n",
    "# ===========================================================\n",
    "# 6. Callbacks\n",
    "# ===========================================================\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"best.h5\", monitor=\"val_accuracy\", save_best_only=True, mode=\"max\", verbose=1),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, verbose=1),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "# ===========================================================\n",
    "# 7. Train the Model\n",
    "# ===========================================================\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=30,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# 8. Evaluation and Visualization\n",
    "# ===========================================================\n",
    "val_loss, val_acc = model.evaluate(val_gen, verbose=0)\n",
    "print(f\"\\nâœ… Final validation accuracy: {val_acc:.2%}\")\n",
    "\n",
    "# Plot training curves\n",
    "def plot_history(hist):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(hist.history['accuracy'], label='Train')\n",
    "    plt.plot(hist.history['val_accuracy'], label='Validation')\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(hist.history['loss'], label='Train')\n",
    "    plt.plot(hist.history['val_loss'], label='Validation')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "# Confusion matrix\n",
    "val_gen.reset()\n",
    "preds = model.predict(val_gen, verbose=0)\n",
    "y_true = val_gen.classes\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[\"dark\", \"medium\", \"light\"])\n",
    "disp.plot(cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix â€“ Validation\")\n",
    "plt.show()\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    rotation_range=30,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    brightness_range=[0.7, 1.4],\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    f\"{extract_path}/train\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    f\"{extract_path}/train\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Class indices:\", train_gen.class_indices)\n",
    "print(\"ðŸ“Š Train distribution:\", dict(Counter(train_gen.classes)))\n",
    "print(\"ðŸ“Š Val distribution:\", dict(Counter(val_gen.classes)))\n",
    "\n",
    "# ===========================================================\n",
    "# 4. Class Weights\n",
    "# ===========================================================\n",
    "class_weights = dict(enumerate(compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_gen.classes),\n",
    "    y=train_gen.classes\n",
    ")))\n",
    "\n",
    "# ===========================================================\n",
    "# 5. Build VGG16 Model (returns model + base_model)\n",
    "# ===========================================================\n",
    "def build_vgg16_model(input_shape=(224, 224, 3), num_classes=3):\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=input_shape))\n",
    "    base_model.trainable = False\n",
    "    x = Flatten()(base_model.output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    out = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=base_model.input, outputs=out)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model, base_model\n",
    "\n",
    "model, base_model = build_vgg16_model()\n",
    "model.summary()\n",
    "\n",
    "# ===========================================================\n",
    "# 6. Callbacks\n",
    "# ===========================================================\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"best.h5\", monitor=\"val_accuracy\", save_best_only=True, mode=\"max\", verbose=1),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, verbose=1),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True, verbose=1),\n",
    "]\n",
    "\n",
    "# ===========================================================\n",
    "# 7. Phase 1: Train Frozen VGG16\n",
    "# ===========================================================\n",
    "history1 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=15,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# 8. Phase 2: Fine-Tune Top Layers of VGG16\n",
    "# ===========================================================\n",
    "# Unfreeze top 2 convolutional blocks (~last 8 layers)\n",
    "for layer in base_model.layers[-8:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    initial_epoch=history1.epoch[-1] + 1,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# 9. Evaluation and Plots\n",
    "# ===========================================================\n",
    "val_loss, val_acc = model.evaluate(val_gen, verbose=0)\n",
    "print(f\"\\nâœ… Final validation accuracy: {val_acc:.2%}\")\n",
    "\n",
    "def plot_history(histories):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for h in histories:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(h.history['accuracy'])\n",
    "        plt.plot(h.history['val_accuracy'])\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend(['Train', 'Val'])\n",
    "    plt.grid()\n",
    "\n",
    "    for h in histories:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(h.history['loss'])\n",
    "        plt.plot(h.history['val_loss'])\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend(['Train', 'Val'])\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history([history1, history2])\n",
    "\n",
    "# Confusion matrix\n",
    "val_gen.reset()\n",
    "preds = model.predict(val_gen, verbose=0)\n",
    "y_true = val_gen.classes\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[\"dark\", \"medium\", \"light\"])\n",
    "disp.plot(cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix â€“ Validation\")\n",
    "plt.show()\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.applications import EfficientNetV2S\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# ===========================================================\n",
    "# 1. Extract dataset\n",
    "# ===========================================================\n",
    "extract_path = os.path.join(os.getcwd(), \"dataset_folder\")\n",
    "if not os.path.exists(extract_path):\n",
    "    with zipfile.ZipFile(\"archive.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(f\"âœ… Dataset extracted to {extract_path}\")\n",
    "else:\n",
    "    print(f\"âœ… Dataset already exists at {extract_path}\")\n",
    "\n",
    "# ===========================================================\n",
    "# 2. Parameters\n",
    "# ===========================================================\n",
    "IMG_SIZE = (300, 300)  # âœ… Best for EfficientNet\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "# ===========================================================\n",
    "# 3. Stronger Augmentation\n",
    "# ===========================================================\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    rotation_range=40,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=(0.6, 1.4),\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    channel_shift_range=30.0,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    os.path.join(extract_path, \"train\"),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    os.path.join(extract_path, \"train\"),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Class indices:\", train_gen.class_indices)\n",
    "print(\"ðŸ“Š Train distribution:\", dict(Counter(train_gen.classes)))\n",
    "print(\"ðŸ“Š Val distribution:\", dict(Counter(val_gen.classes)))\n",
    "\n",
    "# ===========================================================\n",
    "# 4. Class Weights\n",
    "# ===========================================================\n",
    "class_weights = dict(enumerate(compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_gen.classes),\n",
    "    y=train_gen.classes\n",
    ")))\n",
    "\n",
    "# ===========================================================\n",
    "# 5. Build EfficientNetV2S Model\n",
    "# ===========================================================\n",
    "def build_efficientnetv2_model(input_shape=(300, 300, 3), num_classes=3):\n",
    "    base_model = EfficientNetV2S(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=input_shape))\n",
    "    base_model.trainable = False\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    out = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "    lr_schedule = CosineDecay(initial_learning_rate=0.001, decay_steps=10000)\n",
    "    model = Model(inputs=base_model.input, outputs=out)\n",
    "    model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model, base_model\n",
    "\n",
    "model, base_model = build_efficientnetv2_model()\n",
    "model.summary()\n",
    "\n",
    "# ===========================================================\n",
    "# 6. Callbacks\n",
    "# ===========================================================\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"best_efficientnetv2s.h5\", monitor=\"val_accuracy\", save_best_only=True, mode=\"max\", verbose=1),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "# ===========================================================\n",
    "# 7. Phase 1: Train Frozen Model\n",
    "# ===========================================================\n",
    "history1 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=30,  # âœ… More epochs\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# 8. Phase 2: Fine-Tune more layers\n",
    "# ===========================================================\n",
    "for layer in base_model.layers[-60:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=20,  # âœ… More epochs\n",
    "    initial_epoch=history1.epoch[-1] + 1,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# 9. Evaluation\n",
    "# ===========================================================\n",
    "val_loss, val_acc = model.evaluate(val_gen, verbose=0)\n",
    "print(f\"\\nâœ… Final validation accuracy: {val_acc:.2%}\")\n",
    "\n",
    "def plot_history(histories):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for h in histories:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(h.history['accuracy'])\n",
    "        plt.plot(h.history['val_accuracy'])\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend(['Train', 'Val'])\n",
    "    plt.grid()\n",
    "\n",
    "    for h in histories:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(h.history['loss'])\n",
    "        plt.plot(h.history['val_loss'])\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend(['Train', 'Val'])\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history([history1, history2])\n",
    "\n",
    "# ===========================================================\n",
    "# 10. Confusion Matrix\n",
    "# ===========================================================\n",
    "val_gen.reset()\n",
    "preds = model.predict(val_gen, verbose=0)\n",
    "y_true = val_gen.classes\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=list(train_gen.class_indices.keys()))\n",
    "disp.plot(cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix â€“ Validation\")\n",
    "plt.show()\n",
    "\n",
    "# ===========================================================\n",
    "# 11. OPTIONAL TTA Function (+2% boost)\n",
    "# ===========================================================\n",
    "def predict_with_tta(model, generator, tta_steps=5):\n",
    "    preds_tta = np.zeros((generator.samples, generator.num_classes))\n",
    "    for i in range(tta_steps):\n",
    "        generator.reset()\n",
    "        preds_tta += model.predict(generator, verbose=0)\n",
    "    preds_tta /= tta_steps\n",
    "    return preds_tta\n",
    "\n",
    "# Example usage:\n",
    "# preds_tta = predict_with_tta(model, val_gen, tta_steps=5)\n",
    "# y_pred_tta = np.argmax(preds_tta, axis=1)\n",
    "# cm = confusion_matrix(y_true, y_pred_tta)\n",
    "# disp = ConfusionMatrixDisplay(cm, display_labels=list(train_gen.class_indices.keys()))\n",
    "# disp.plot(cmap=\"Blues\", colorbar=False)\n",
    "# plt.title(\"TTA Confusion Matrix â€“ Validation\")\n",
    "# plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Skinâ€‘Tone Classification with EfficientNetV2â€‘S\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 1. Imports\n",
    "\n",
    "import os, zipfile, numpy as np, matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D,\n",
    "                                     Dense, Dropout, BatchNormalization)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.applications import EfficientNetV2S\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "\n",
    "\n",
    "# 2. Dataset (zip â‡’ folder if needed)\n",
    "\n",
    "extract_path = os.path.join(os.getcwd(), \"dataset_folder\")\n",
    "if not os.path.exists(extract_path):\n",
    "    with zipfile.ZipFile(\"archive.zip\") as zf:\n",
    "        zf.extractall(extract_path)\n",
    "    print(f\"Dataset extracted to {extract_path}\")\n",
    "else:\n",
    "    print(f\"âœ… Dataset already exists at {extract_path}\")\n",
    "\n",
    "\n",
    "# 3. Parameters & Augmentation\n",
    "\n",
    "IMG_SIZE   = (300, 300)              # best for EfficientNetV2â€‘S\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT  = 0.20\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    rotation_range=40,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=(0.6, 1.4),\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    channel_shift_range=30.0,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    os.path.join(extract_path, \"train\"),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    os.path.join(extract_path, \"train\"),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(\"Class indices:\", train_gen.class_indices)\n",
    "print(\"Train distribution:\", dict(Counter(train_gen.classes)))\n",
    "print(\"Val distribution:\", dict(Counter(val_gen.classes)))\n",
    "\n",
    "\n",
    "# 4. Class Weights (handle any imbalance)\n",
    "\n",
    "class_weights = dict(\n",
    "    enumerate(\n",
    "        compute_class_weight(\n",
    "            class_weight=\"balanced\",\n",
    "            classes=np.unique(train_gen.classes),\n",
    "            y=train_gen.classes,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Build EfficientNetV2â€‘S model\n",
    "\n",
    "def build_model(input_shape=(300, 300, 3), n_classes=3):\n",
    "    base = EfficientNetV2S(\n",
    "        weights=\"imagenet\", include_top=False, input_tensor=Input(shape=input_shape)\n",
    "    )\n",
    "    base.trainable = False                              # Phaseâ€‘1 frozen\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base.output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    out = Dense(n_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    lr_sched = CosineDecay(initial_learning_rate=1e-3, decay_steps=10_000)\n",
    "    model = Model(base.input, out)\n",
    "    model.compile(optimizer=Adam(lr_sched),\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model, base\n",
    "\n",
    "model, base_model = build_model()\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# 6. Callbacks\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"best_efficientnetv2s.h5\",\n",
    "                    monitor=\"val_accuracy\",\n",
    "                    save_best_only=True,\n",
    "                    mode=\"max\",\n",
    "                    verbose=1),\n",
    "    EarlyStopping(monitor=\"val_loss\",\n",
    "                  patience=6,\n",
    "                  restore_best_weights=True,\n",
    "                  verbose=1),\n",
    "]\n",
    "\n",
    "\n",
    "# 7. Phaseâ€‘1 Training (feature extractor)\n",
    "\n",
    "history1 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=30,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "# 8. Phaseâ€‘2 Fineâ€‘Tuning (unfreeze last 60 layers)\n",
    "\n",
    "for layer in base_model.layers[-60:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=Adam(1e-5),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=20,\n",
    "    initial_epoch=len(history1.epoch),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "# 9. Metrics Plot\n",
    "\n",
    "def plot_history(histories):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    # --- Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for h in histories:\n",
    "        plt.plot(h.history[\"accuracy\"])\n",
    "        plt.plot(h.history[\"val_accuracy\"])\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    # --- Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for h in histories:\n",
    "        plt.plot(h.history[\"loss\"])\n",
    "        plt.plot(h.history[\"val_loss\"])\n",
    "    plt.title(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history([history1, history2])\n",
    "\n",
    "\n",
    "# 10. Evaluation + Confusion Matrix (labelâ€‘remap)\n",
    "\n",
    "val_loss, val_acc = model.evaluate(val_gen, verbose=0)\n",
    "print(f\"Final validation accuracy: {val_acc:.2%}\")\n",
    "\n",
    "# â”€â”€ Predictions\n",
    "val_gen.reset()\n",
    "preds   = model.predict(val_gen, verbose=0)\n",
    "y_true  = val_gen.classes\n",
    "y_pred  = np.argmax(preds, axis=1)\n",
    "\n",
    "# â”€â”€ Humanâ€‘readable label mapping\n",
    "orig_to_readable = {'Black': 'dark', 'Brown': 'medium', 'White': 'light'}\n",
    "readable_labels  = [orig_to_readable[k] for k in train_gen.class_indices.keys()]\n",
    "\n",
    "cm   = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=readable_labels)\n",
    "disp.plot(cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix â€“ Validation\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 11. (Optional) Testâ€‘Time Augmentation helper\n",
    "\n",
    "def predict_with_tta(model, generator, tta_steps=5):\n",
    "    \"\"\"Return averaged probabilities across `tta_steps` passes.\"\"\"\n",
    "    acc_probs = np.zeros((generator.samples, generator.num_classes))\n",
    "    for _ in range(tta_steps):\n",
    "        generator.reset()\n",
    "        acc_probs += model.predict(generator, verbose=0)\n",
    "    return acc_probs / tta_steps\n",
    "\n",
    "!pip install tensorflow==2.12.0 --upgrade\n",
    "\n",
    "\"\"\"\n",
    "EfficientNetV2â€‘S skinâ€‘tone classifier (improved)\n",
    "Folders: dataset_folder/train/Black, Brown, White\n",
    "Humanâ€‘readable labels: dark / medium / light\n",
    "\"\"\"\n",
    "\n",
    "# ===========================================================\n",
    "# 1. Imports & dataset unzip (same as before)\n",
    "# ===========================================================\n",
    "import os, zipfile, numpy as np, matplotlib.pyplot as plt, tensorflow as tf\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, Dense,\n",
    "                                     Dropout, BatchNormalization)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, EarlyStopping,\n",
    "                                        ReduceLROnPlateau)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "from tensorflow.keras.applications import EfficientNetV2S\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "\n",
    "extract_path = os.path.join(os.getcwd(), \"dataset_folder\")\n",
    "if not os.path.exists(extract_path):\n",
    "    with zipfile.ZipFile(\"archive.zip\") as zf:\n",
    "        zf.extractall(extract_path)\n",
    "\n",
    "# ===========================================================\n",
    "# 2. Parameters\n",
    "# ===========================================================\n",
    "IMG_SIZE   = (300, 300)\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT  = 0.2\n",
    "\n",
    "# ===========================================================\n",
    "# 3. Moderated augmentation\n",
    "# ===========================================================\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    rotation_range=30,\n",
    "    shear_range=0.25,\n",
    "    zoom_range=(0.75, 1.25),\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    channel_shift_range=20.0,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    os.path.join(extract_path, \"train\"),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    os.path.join(extract_path, \"train\"),\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(\"âœ… Class indices:\", train_gen.class_indices)\n",
    "print(\"ðŸ“Š Train distribution:\", dict(Counter(train_gen.classes)))\n",
    "print(\"ðŸ“Š Val   distribution:\", dict(Counter(val_gen.classes)))\n",
    "\n",
    "# ===========================================================\n",
    "# 4. Class weights\n",
    "# ===========================================================\n",
    "class_weights = dict(\n",
    "    enumerate(\n",
    "        compute_class_weight(\"balanced\",\n",
    "                             classes=np.unique(train_gen.classes),\n",
    "                             y=train_gen.classes)\n",
    "    )\n",
    ")\n",
    "\n",
    "# ===========================================================\n",
    "# 5. Focalâ€‘loss with label smoothing\n",
    "# ===========================================================\n",
    "def focal_loss(alpha=1.0, gamma=2.0, label_smoothing=0.05):\n",
    "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true   = y_true * (1 - label_smoothing) + label_smoothing / 3\n",
    "        y_pred   = tf.clip_by_value(y_pred, 1e-7, 1-1e-7)\n",
    "        cross_ce = -y_true * tf.math.log(y_pred)\n",
    "        weight   = alpha * tf.pow(1 - y_pred, gamma)\n",
    "        return tf.reduce_sum(weight * cross_ce, axis=-1)\n",
    "    return loss\n",
    "\n",
    "alpha_vec = [1.0, 3.0, 1.0]      # amplify medium class (index 1)\n",
    "\n",
    "# ===========================================================\n",
    "# 6. Build EfficientNetV2â€‘S model\n",
    "# ===========================================================\n",
    "def build_model():\n",
    "    base = EfficientNetV2S(weights=\"imagenet\",\n",
    "                           include_top=False,\n",
    "                           input_tensor=Input(shape=(*IMG_SIZE, 3)))\n",
    "    base.trainable = False\n",
    "    x = GlobalAveragePooling2D()(base.output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    out = Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "    lr_sched = CosineDecayRestarts(initial_learning_rate=1e-3,\n",
    "                                   first_decay_steps=5*len(train_gen),\n",
    "                                   t_mul=2.0, m_mul=0.5)\n",
    "\n",
    "    model = Model(base.input, out)\n",
    "    model.compile(optimizer=Adam(lr_sched),\n",
    "                  loss=focal_loss(alpha_vec, gamma=2.0, label_smoothing=0.05),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model, base\n",
    "\n",
    "model, base_model = build_model()\n",
    "model.summary()\n",
    "\n",
    "# ===========================================================\n",
    "# 7. Callbacks\n",
    "# ===========================================================\n",
    "cbs = [\n",
    "    ModelCheckpoint(\"best_effnetv2s.h5\", monitor=\"val_accuracy\",\n",
    "                    save_best_only=True, mode=\"max\", verbose=1),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=8,\n",
    "                  restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2,\n",
    "                      patience=4, min_lr=1e-6, verbose=1),\n",
    "]\n",
    "\n",
    "# ===========================================================\n",
    "# 8. Phaseâ€‘1: frozen feature extractor\n",
    "# ===========================================================\n",
    "history1 = model.fit(train_gen,\n",
    "                     validation_data=val_gen,\n",
    "                     epochs=20,\n",
    "                     class_weight=class_weights,\n",
    "                     callbacks=cbs,\n",
    "                     verbose=1)\n",
    "\n",
    "# ===========================================================\n",
    "# 9. Phaseâ€‘2: fineâ€‘tune last 120 layers\n",
    "# ===========================================================\n",
    "for layer in base_model.layers[-120:]:\n",
    "    layer.trainable = True\n",
    "model.compile(optimizer=Adam(3e-6),\n",
    "              loss=focal_loss(alpha_vec, 2.0, 0.05),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history2 = model.fit(train_gen,\n",
    "                     validation_data=val_gen,\n",
    "                     epochs=25,\n",
    "                     initial_epoch=len(history1.epoch),\n",
    "                     class_weight=class_weights,\n",
    "                     callbacks=cbs,\n",
    "                     verbose=1)\n",
    "\n",
    "# ===========================================================\n",
    "# 10. Evaluation & plots\n",
    "# ===========================================================\n",
    "val_loss, val_acc = model.evaluate(val_gen, verbose=0)\n",
    "print(f\"\\nðŸ”Ž Final validation accuracy: {val_acc:.2%}\")\n",
    "\n",
    "# training curves\n",
    "def plot_hist(*hists):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    # accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for h in hists:\n",
    "        plt.plot(h.history[\"accuracy\"])\n",
    "        plt.plot(h.history[\"val_accuracy\"])\n",
    "    plt.title(\"Accuracy\"); plt.grid(True)\n",
    "    # loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for h in hists:\n",
    "        plt.plot(h.history[\"loss\"])\n",
    "        plt.plot(h.history[\"val_loss\"])\n",
    "    plt.title(\"Loss\"); plt.grid(True)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_hist(history1, history2)\n",
    "\n",
    "# ===========================================================\n",
    "# 11. Confusion matrix (renamed)\n",
    "# ===========================================================\n",
    "val_gen.reset()\n",
    "preds   = model.predict(val_gen, verbose=0)\n",
    "y_true  = val_gen.classes\n",
    "y_pred  = np.argmax(preds, axis=1)\n",
    "\n",
    "label_map = {\"Black\": \"dark\", \"Brown\": \"medium\", \"White\": \"light\"}\n",
    "readable  = [label_map[k] for k in train_gen.class_indices.keys()]\n",
    "\n",
    "cm   = confusion_matrix(y_true, y_pred)\n",
    "ConfusionMatrixDisplay(cm, display_labels=readable).plot(\n",
    "    cmap=\"Blues\", colorbar=False)\n",
    "plt.title(\"Confusion Matrix â€“ Validation\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
